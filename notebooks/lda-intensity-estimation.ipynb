{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kd Analysis\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#                                                                                  #\n",
    "# Leave these values blank to let the script determine them automatically.         #\n",
    "# Only set them if something goes wrong!                                           #\n",
    "#                                                                                  #\n",
    "####################################################################################\n",
    "chip_id = ''\n",
    "target_name = ''\n",
    "neg_control_target_name = ''\n",
    "all_channels = []\n",
    "data_channel = ''\n",
    "target_sequence_file = \"/shared/targets.yml\"\n",
    "nonneg_lda_weights_fpath = '/shared/yeast_beast_LDA_weights.txt'  # for microscope 3\n",
    "# nonneg_lda_weights_fpath = '/shared/bLDA_coef_nonneg.txt'  # for microscope 2 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import yaml\n",
    "from champ import initialize\n",
    "\n",
    "def determine_chip_id(override_value):\n",
    "    # let the user override this method with a manually-specified value\n",
    "    if override_value:\n",
    "        return override_value\n",
    "    \n",
    "    chip_regex = re.compile(r'.*?(SA\\d{5}).*?')\n",
    "    filenames = glob.glob('*')\n",
    "    candidates = set()\n",
    "    \n",
    "    # look through all the filenames in this directory and look for anything that looks like a sequencing run ID, which we use as chip IDs\n",
    "    for filename in filenames:\n",
    "        match = chip_regex.search(filename)\n",
    "        if match:\n",
    "            candidates.add(match.group(1))\n",
    "    # there should be a unique value unless someone had an unfortunate choice of filenames\n",
    "    if len(candidates) == 1:\n",
    "        return list(candidates)[0]\n",
    "    raise ValueError(\"We were unable to automatically determine the chip ID based on filenames, you'll need to set it manually.\")\n",
    "\n",
    "\n",
    "def load_config_value(item_name, override_value):\n",
    "    # let the user override this method with a manually-specified value\n",
    "    if override_value:\n",
    "        return override_value\n",
    "    try:\n",
    "        with open(\"champ.yml\") as f:\n",
    "            config = yaml.load(f)\n",
    "            return config[item_name]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise ValueError(\"We could not determine the {item_name} from champ.yml. Make sure you have a configuration file and that the value is set.\".format(item_name=item_name))\n",
    "\n",
    "        \n",
    "def determine_data_channel(all_channels, alignment_channel):\n",
    "    alignment_channel = set([alignment_channel])\n",
    "    all_channels = set(map(str, all_channels))\n",
    "    data_channel = all_channels - alignment_channel\n",
    "    if len(data_channel) == 1:\n",
    "        # There are two channels, so we return the one that isn't the alignment channel\n",
    "        return list(data_channel)[0]\n",
    "    if not data_channel:\n",
    "        # There is only one channel, so alignment markers and data are in the same channel\n",
    "        return list(alignment_channel)[0]\n",
    "    raise ValueError(\"Could not determine data channel. You'll need to set this manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_id = determine_chip_id(chip_id)\n",
    "target_name = load_config_value('perfect_target_name', target_name)\n",
    "neg_control_target_name = load_config_value('neg_control_target_name', neg_control_target_name)\n",
    "all_channels = list(map(str, initialize.determine_channel_names('.'))) if not all_channels else all_channels\n",
    "alignment_channel = load_config_value('alignment_channel', data_channel)\n",
    "data_channel = determine_data_channel(all_channels, alignment_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict, Counter\n",
    "from IPython.display import HTML, Image\n",
    "from champ import misc, intensity, initialize, seqtools, adapters_cython\n",
    "import yaml\n",
    "\n",
    "read_name_dir = os.path.join('/shared', chip_id, 'read_names')\n",
    "read_names_by_seq_fpath = os.path.join(read_name_dir, 'read_names_by_seq.txt')\n",
    "out_fname = 'LDA_intensity_scores.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Specific Section\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(target_sequence_file) as f:\n",
    "    targets = yaml.load(f)\n",
    "\n",
    "target = targets[target_name]\n",
    "neg_control_target = targets[neg_control_target_name]\n",
    "datadir = 'results'\n",
    "figdir = 'figs'\n",
    "for dpath in [figdir, datadir]:\n",
    "    if not os.path.isdir(dpath):\n",
    "        os.makedirs(dpath)\n",
    "out_fpath = os.path.join(datadir, out_fname)\n",
    "\n",
    "print 'Chip ID:', chip_id\n",
    "print 'Target \"{}\":'.format(target_name), target\n",
    "print 'Neg control target \"{}\":'.format(neg_control_target_name), neg_control_target\n",
    "print 'Channels:', all_channels\n",
    "print 'Protein channel:', data_channel\n",
    "print 'Output file:', out_fpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence of Interest Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_seqs = set()\n",
    "    \n",
    "stretch = set()\n",
    "for i in range(1, len(target)+1):\n",
    "    stretch.update(seqtools.get_stretch_of_complement_seqs(target, i))\n",
    "insertions = set()\n",
    "for i in range(1, 3):\n",
    "    insertions.update(seqtools.get_contiguous_insertion_seqs(target, i))\n",
    "for i in range(1, 3):\n",
    "    insertions.update(seqtools.get_insertion_seqs(target, i))   \n",
    "deletions = set()\n",
    "for i in range(1, 3):\n",
    "    deletions.update(seqtools.get_deletion_seqs(target, i))\n",
    "mismatches = set()\n",
    "for i in range(1, 3):\n",
    "    mismatches.update(seqtools.get_mismatch_seqs(target, i))\n",
    "six_n_pam = seqtools.get_randomized_pam_seqs(target, 4, 6)\n",
    "other_targets = set()\n",
    "for s in targets.values():\n",
    "    other_targets.add(s)\n",
    "\n",
    "interesting_seqs.update(other_targets)\n",
    "interesting_seqs.update(stretch)\n",
    "interesting_seqs.update(insertions)\n",
    "interesting_seqs.update(deletions)\n",
    "interesting_seqs.update(mismatches)\n",
    "interesting_seqs.update(six_n_pam)\n",
    "\n",
    "print(\"Interesting sequences: %d\" % len(interesting_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Interesting Sequences Files\n",
    "\n",
    "For some reason, the `read_names_by_seq.txt` file often contains sequences with extra bases on either end of the sequence we actually care about. Which is to say, the reads are not being parsed properly. This wasn't happening before and I don't know what's changed. Regardless, here we go through it, and check every single sequence in that file to see if it contains a sequence of interest as a substring. This way, we generate a custom file that contains an exact mapping between read names and interesting sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from champ.seqtools import build_interesting_sequences\n",
    "interesting_read_names_filename = os.path.join(read_name_dir, 'interesting_{target_name}_reads_by_seq.txt'.format(target_name=target_name))\n",
    "if os.path.exists(interesting_read_names_filename):\n",
    "    # No need to recalculate, we can just load this from disk\n",
    "    interesting_read_names = {}\n",
    "    with open(interesting_read_names_filename) as f:\n",
    "        for line in f:\n",
    "            line = line.split(\"\\t\")\n",
    "            sequence = line[0]\n",
    "            read_names = line[1:]\n",
    "            interesting_read_names[sequence] = read_names\n",
    "else:\n",
    "    interesting_read_names = build_interesting_sequences(read_names_by_seq_fpath, interesting_seqs)\n",
    "    with open(interesting_read_names_filename, 'w') as f:\n",
    "        for sequence, read_names in interesting_read_names.items():\n",
    "            f.write(\"%s\\t%s\\n\" % (sequence, \"\\t\".join(read_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide how many insertions or deletions to allow\n",
    "min_len = len(target) - 3\n",
    "max_len = len(target) + 3\n",
    "max_ham = 7\n",
    "\n",
    "def is_interesting_seq(seq):\n",
    "    if seq in interesting_seqs:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_read_name_fpath = os.path.join(read_name_dir, 'all_read_names.txt')\n",
    "target_read_name_fpath = os.path.join(read_name_dir, 'target_{}_read_names.txt'.format(target_name.lower()))\n",
    "perfect_target_read_name_fpath = os.path.join(read_name_dir, 'perfect_target_{}_read_names.txt'.format(target_name.lower()))\n",
    "neg_control_target_read_name_fpath = os.path.join(read_name_dir, 'perfect_target_{}_read_names.txt'.format(neg_control_target_name.lower()))\n",
    "phiX_read_name_fpath = os.path.join(read_name_dir, 'phix_read_names.txt')\n",
    "\n",
    "all_read_names = set(line.strip() for line in open(all_read_name_fpath))\n",
    "print(\"All read names: %d\" % len(all_read_names))\n",
    "target_read_names = set(line.strip() for line in open(target_read_name_fpath))\n",
    "print(\"Target read names: %d\" % len(target_read_names))\n",
    "perfect_target_read_names = set(line.strip() for line in open(perfect_target_read_name_fpath))\n",
    "print(\"Perfect target read names: %d\" % len(perfect_target_read_names))\n",
    "neg_control_target_read_names = set(line.strip() for line in open(neg_control_target_read_name_fpath))\n",
    "print(\"Negative control read names: %d\" % len(neg_control_target_read_names))\n",
    "phiX_read_names = set(line.strip() for line in open(phiX_read_name_fpath))\n",
    "print(\"Phix read names: %d\" % len(phiX_read_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_fpaths = glob.glob('*.h5')\n",
    "i = 0\n",
    "while i < len(h5_fpaths):\n",
    "    if 'PhiX' in h5_fpaths[i] or 'chip' in h5_fpaths[i]:\n",
    "        h5_fpaths.pop(i)\n",
    "    else:\n",
    "        i += 1\n",
    "h5_fpaths.sort(key=misc.parse_concentration)\n",
    "for fpath in h5_fpaths:\n",
    "    print misc.parse_concentration(fpath), fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dirs = [\n",
    "    os.path.join('results', \n",
    "                 os.path.splitext(os.path.basename(h5_fpath))[0])\n",
    "    for h5_fpath in h5_fpaths\n",
    "]\n",
    "for d in results_dirs:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Loading data...'\n",
    "int_scores = intensity.IntensityScores(h5_fpaths)\n",
    "int_scores.get_LDA_scores(results_dirs, nonneg_lda_weights_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print 'Normalizing data...'\n",
    "int_scores.normalize_scores()\n",
    "print 'Done normalizing.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.plot_aligned_images('br', 'o*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.plot_normalization_constants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.print_reads_per_channel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_num_ims_cutoff = len(h5_fpaths) - 3\n",
    "int_scores.build_good_read_names(good_num_ims_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_read_names = int_scores.good_read_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_perfect_read_names = perfect_target_read_names & good_read_names\n",
    "print 'Good Reads:', len(good_read_names)\n",
    "print 'Good Perfect Reads:', len(good_perfect_read_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_scores.build_score_given_read_name_given_channel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Collating by Sequence\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find only read names with cascade scores\n",
    "print(\"Starting\")\n",
    "aligned_read_names = []\n",
    "for h5_fpath in h5_fpaths:\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    for d in int_scores.scores[h5_fpath][data_channel].values():\n",
    "        for read_name in d.keys():\n",
    "            aligned_read_names.append(read_name)\n",
    "aligned_read_names = set(aligned_read_names)\n",
    "print '\\nAligned reads in protein channel:', len(aligned_read_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    max_ham\n",
    "except:\n",
    "    max_ham = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Collating Reads by Sequence'\n",
    "interesting_reads = seqtools.build_read_names_given_seq(target,\n",
    "                                                        interesting_read_names_filename,\n",
    "                                                        aligned_read_names,\n",
    "                                                        is_interesting_seq,\n",
    "                                                        max_ham)\n",
    "\n",
    "print(len(interesting_reads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_reads[neg_control_target].update(neg_control_target_read_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter reads with outlier intensities for their sequence identity, and filter sequences with fewer than 5 reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_reads = 5\n",
    "tukey_contant = 1.5  # Read acceptabale if in range [q1 - tukey_contant * iqr, q3 + tukey_contant * iqr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Filtering reads by intensity and seqs by final read count'\n",
    "interesting_seqs = set()\n",
    "for i, (seq, read_names) in enumerate(interesting_reads.items()):\n",
    "    if i % 10000 == 0:\n",
    "        sys.stdout.write('.')\n",
    "        sys.stdout.flush()\n",
    "    if len(read_names) < min_reads:\n",
    "        continue\n",
    "    for h5_fpath in h5_fpaths:\n",
    "        if data_channel not in int_scores.score_given_read_name_in_channel[h5_fpath]:\n",
    "            continue\n",
    "        score_given_read_name = int_scores.score_given_read_name_in_channel[h5_fpath][data_channel]\n",
    "        intensities = [\n",
    "            score_given_read_name[read_name]\n",
    "            for read_name in read_names\n",
    "            if read_name in score_given_read_name\n",
    "        ]\n",
    "        if len(intensities) < min_reads:\n",
    "            continue\n",
    "        q1 = np.percentile(intensities, 25)\n",
    "        q3 = np.percentile(intensities, 75)\n",
    "        iqr = q3 - q1\n",
    "        min_range, max_range = (q1 - tukey_contant * iqr, q3 + tukey_contant * iqr)\n",
    "        new_read_names = set()\n",
    "        for read_name in read_names:\n",
    "            try:\n",
    "                if min_range <= score_given_read_name[read_name] <= max_range:\n",
    "                    new_read_names.add(read_name)\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "    interesting_reads[seq] = new_read_names\n",
    "    if len(new_read_names) >= min_reads:\n",
    "        interesting_seqs.add(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "seqtools.plot_library_comp_by_hamming_distance(ax,\n",
    "                                               target,\n",
    "                                               max_ham,\n",
    "                                               min_reads,\n",
    "                                               interesting_reads,\n",
    "                                               interesting_seqs)\n",
    "ax.set_title('Target {} Library'.format(target_name), fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Negative Control Seqs:', len(interesting_reads[neg_control_target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentrations = map(misc.parse_concentration, h5_fpaths)\n",
    "print 'Concentrations:'\n",
    "concentrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trait_name = 'concentration_pM'\n",
    "trait_list = concentrations\n",
    "attrs_dict = {\n",
    "    'target': target, \n",
    "    'target_name': target_name,\n",
    "    'neg_control_target': neg_control_target,\n",
    "    'neg_control_target_name': neg_control_target_name,\n",
    "}\n",
    "\n",
    "int_scores.write_values_by_seq(\n",
    "    course_trait_name=trait_name,\n",
    "    course_trait_list=trait_list,\n",
    "    h5_fpaths=h5_fpaths,\n",
    "    attrs_dict=attrs_dict,\n",
    "    channel_of_interest=data_channel,\n",
    "    seqs_of_interest=interesting_seqs,\n",
    "    read_names_given_seq=interesting_reads,\n",
    "    out_fpath=out_fpath,       \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
